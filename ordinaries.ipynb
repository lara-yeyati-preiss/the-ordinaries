{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29debe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing libraries that let pandas read parquet files\n",
    "\n",
    "%pip install -U pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Path class so you can create Path objects directly\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pandas and reading the parquet file\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(\"si_revwar.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b95b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking table to confirm it loaded correctly\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7cfd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking what Python types are in the column indexed_object_types\n",
    "df[\"indexed_object_types\"].map(type).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bebfcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are only NoneType values in the column, so I will drop those rows\n",
    "df = df[df[\"indexed_object_types\"].map(type) != type(None)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa325b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rest of the values are strings that look like JSON lists\n",
    "# I want to parse them from strings to actual JSON lists\n",
    "# (for example, \"[\"note\",\"voucher\"]\"\" should become just [\"note\", \"voucher\"])\n",
    "import json\n",
    "\n",
    "def parse_indexed_object_types(cell):\n",
    "    \"\"\"\n",
    "    converts a string that looks like a JSON list into a real JSON list\n",
    "    \"\"\"\n",
    "    return json.loads(cell)          # parses original string into a JSON list\n",
    "\n",
    "# creating new column with the parsed object types\n",
    "df[\"object_types_list\"] = df[\"indexed_object_types\"].apply(parse_indexed_object_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c786c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking what Python types are in the new column object_types_list now (they should all be lists)\n",
    "df[\"object_types_list\"].map(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the lists in object_types_list\n",
    "\n",
    "import re # for regular expressions\n",
    "import unicodedata # for handling Unicode characters consistently\n",
    "\n",
    "def normalize_token_basic(t):\n",
    "    s = unicodedata.normalize(\"NFKC\", str(t)) #transforms the string into a standardized Unicode form:\n",
    "    s = s.lower().strip() #turns everything to lowercase and removes leading/trailing whitespace\n",
    "    s = re.sub(r\"[\\u2010-\\u2015\\-_\\/]+\", \" \", s) #replaces various dashes, underscores, and slashes with a single space\n",
    "    return s #returns the cleaned-up string\n",
    "df[\"object_types_norm\"] = df[\"object_types_list\"].apply(\n",
    "    lambda items: [normalize_token_basic(x) for x in items]\n",
    ")   # loops through each row in the column and passes the value in that row to the normalization function, applying it to each item in the lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39592ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unique full-cell values in column object_types_norm\n",
    "\n",
    "# 1. converting each list into a string so pandas can compare easily\n",
    "df[\"object_types_str\"] = df[\"object_types_norm\"].apply(str)\n",
    "\n",
    "# 2. counting unique values and their frequencies\n",
    "counts_df = (\n",
    "    df[\"object_types_str\"]\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"object_types\", \"object_types_str\": \"count\"})\n",
    ")\n",
    "\n",
    "# 3. inspecting top values\n",
    "display(counts_df.head(50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ead0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are a lot of variations of the same object types\n",
    "# exporting the CSV to review the different object types and define a grouping strategy using an LLM\n",
    "# canonicalization criteria provided to the LLM:\n",
    "# 1. normalize spelling\n",
    "# 2. merge synonyms under a single name (e.g., \"pipe tampers\", \"pipe, tobaccos\" → \"pipes\")\n",
    "# 2. remove overly specific qualifiers (e.g., \"silver spoon\", \"table spoon\" → \"spoons\")\n",
    "# 3. collapse minor variants into parent category (e.g., \"dress, 1 piece\", \"dress fragments\" → \"dresses\")\n",
    "# 4. keep distinct when functionally differs (e.g., \"box, stamps\"  ≠ \"box, doughs\")\n",
    "# 5. use plural form consistently (e.g., \"painting\" → \"paintings\")\n",
    "\n",
    "counts_df.to_csv(\"unique_object_types_fullcells.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3617dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and displaying the curated canonicalization mapping\n",
    "\n",
    "mapping_df = pd.read_csv(\"canonical_map.csv\")\n",
    "\n",
    "# this CSV has two key columns:\n",
    "# - \"original_fullcell\": the original unique string from the database\n",
    "# - \"canonical_plural\": the standardized category, in plural form\n",
    "\n",
    "display(mapping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b5fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning that csv into a dictionary\n",
    "\n",
    "mapping_dict = pd.Series(mapping_df.canonical_plural.values, index=mapping_df.original_fullcell).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113708e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the dictionary to the dataframe\n",
    "# first, I need the column in the same format as \"original_fullcell\", which means\n",
    "# converting the lists back to strings\n",
    "\n",
    "df[\"object_types_str\"] = df[\"object_types_norm\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d46652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping each row to its canonical category using the dictionary\n",
    "# this function goes through every element in the column object_types_str from df, \n",
    "# looks them up in the mapping_dict dictionary and creates the new column canonical_type corresponding element form the  column from \n",
    "\n",
    "df[\"canonical_type\"] = df[\"object_types_str\"].map(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda34abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how many rows successfully mapped\n",
    "\n",
    "print(\"rows mapped:\", df[\"canonical_type\"].notna().sum())\n",
    "print(\"rows unmapped:\", df[\"canonical_type\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the df to see if the column was added correctly\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fde85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the first 50 rows with original and canonical categories side by side\n",
    "\n",
    "df[[\"object_types_str\", \"canonical_type\"]].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd975e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the frequency of the top 40 canonical type created\n",
    "\n",
    "df[\"canonical_type\"].value_counts().head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8338f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting unique canonical categories with counts\n",
    "\n",
    "# 1. counting frequency of each canonical category\n",
    "counts = (\n",
    "    df[\"canonical_type\"]\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"canonical_type\", \"canonical_type\": \"count\"})\n",
    ")\n",
    "\n",
    "# 2. inspecting the top categories\n",
    "print(counts.head(20))\n",
    "\n",
    "# 3. exporting to CSV for review, to define key verb families with a mixed strategy: \n",
    "# overall grouping using self-defined action families prompted to an LLM, followed by manual review and correction\n",
    "\n",
    "counts.to_csv(\"canonical_types_counts.csv\", index=False)\n",
    "\n",
    "# proposed action families:\n",
    "# 1. Commemorate & symbolize \n",
    "# 2. Decorate & furnish Dress & accessorize \n",
    "# 3. Eat, cook & drink \n",
    "# 4. Fight \n",
    "# 5. Heal & care \n",
    "# 6. Ignite & manage fire \n",
    "# 7. Measure & navigate \n",
    "# 8. Perform music \n",
    "# 9. Play \n",
    "# 10. Read, write & record \n",
    "# 11. Smoke \n",
    "# 12. Textile making \n",
    "# 13. Work & build \n",
    "# 14. Worship \n",
    "# 15. Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cea37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and displaying the curated object - verb family mapping\n",
    "# it has two key columns: canonical_type and action_family\n",
    "map_df = pd.read_csv(\"object_verb_mapping.csv\")\n",
    "display(map_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02197196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating df_with_action by taking the original df (that already has the canonical_type column) \n",
    "# and merging it with map_df, based on the canonical_type\n",
    "\n",
    "df_with_action = df.merge(\n",
    "    map_df,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# displaying df_with_actions\n",
    "display(df_with_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d38a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if all elements have been assigned an action\n",
    "print(df_with_action['action_family'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5cafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_action.to_csv(\"final_mapped_database.csv\")\n",
    "display(df_with_action)\n",
    "# by reviewing the data, I notice that the action family defined as \"Contain, store & preserve\" contains mostly jars\n",
    "# however, by looking at more details in other columns it is clear that these jars where used as pharmaceutical containers\n",
    "# I re-tag the objects inside this action family to better match their purpose (\"Heal & care\")\n",
    "# I also manually assign action families to other objects that were tagged as \"Other\".\n",
    "# Importing the curated file with these changes and other manual re-tagging edits\n",
    "df_with_action = pd.read_csv(\"final_mapped_database_manual.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the auto-mapped vs. manually-mapped CSVs to document every change made by hand,\n",
    "# and exporting those differences as 'override' tables that can be reapplied later\n",
    "\n",
    "\n",
    "# 1) input / output paths\n",
    "\n",
    "# input\n",
    "auto_file = Path(\"final_mapped_database.csv\")\n",
    "manual_file = Path(\"final_mapped_database_manual.csv\")\n",
    "\n",
    "# output\n",
    "overrides_canonical_file = Path(\"manual_overrides_canonical.csv\")\n",
    "overrides_action_file = Path(\"manual_overrides_action.csv\")\n",
    "audit_rowlevel_file = Path(\"manual_overrides_rowlevel.csv\")\n",
    "\n",
    "# 2) read both CSVs\n",
    "auto_df = pd.read_csv(auto_file)\n",
    "manual_df = pd.read_csv(manual_file)\n",
    "\n",
    "# 3) fixed join key and other important columns\n",
    "id_col = \"EDANurl\"  # Smithsonian’s unique record URL\n",
    "group_col = \"object_types_str\"  # original object-type label\n",
    "fields_to_compare = [\"canonical_type\", \"action_family\"]\n",
    "\n",
    "# 4) merge so each row shows auto + manual side by side\n",
    "merged = auto_df[[id_col, group_col] + fields_to_compare] \\\n",
    "           .merge(\n",
    "               manual_df[[id_col, group_col] + fields_to_compare],\n",
    "               on=[id_col, group_col],\n",
    "               suffixes=(\"_auto\", \"_manual\"),\n",
    "               how=\"inner\"\n",
    "           )\n",
    "\n",
    "# 5) detect changed rows\n",
    "changed_mask = False\n",
    "for f in fields_to_compare:\n",
    "    changed = merged[f\"{f}_auto\"].astype(str).fillna(\"\") != \\\n",
    "              merged[f\"{f}_manual\"].astype(str).fillna(\"\")\n",
    "    changed_mask = changed if isinstance(changed_mask, bool) else (changed_mask | changed)\n",
    "\n",
    "changed_rows = merged[changed_mask].copy()\n",
    "changed_rows.to_csv(audit_rowlevel_file, index=False)\n",
    "\n",
    "# 6) build one-to-one overrides by original label\n",
    "def build_override(field):\n",
    "    a, m = f\"{field}_auto\", f\"{field}_manual\"\n",
    "    g = merged.groupby(group_col)[[a, m]] \\\n",
    "              .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else s.iloc[0]) \\\n",
    "              .reset_index().rename(columns={a: \"auto\", m: \"manual\"})\n",
    "    return g[g[\"auto\"].astype(str).fillna(\"\") != g[\"manual\"].astype(str).fillna(\"\")]\n",
    "\n",
    "ov_canonical = build_override(\"canonical_type\")\n",
    "ov_action    = build_override(\"action_family\")\n",
    "\n",
    "if not ov_canonical.empty:\n",
    "    ov_canonical.rename(columns={group_col: \"original_label\",\n",
    "                                 \"manual\": \"canonical_type_manual\"}, inplace=True)\n",
    "    ov_canonical[[\"original_label\", \"canonical_type_manual\"]] \\\n",
    "        .to_csv(overrides_canonical_file, index=False)\n",
    "\n",
    "if not ov_action.empty:\n",
    "    ov_action.rename(\n",
    "        columns={\n",
    "            group_col: \"object_type\",       # keep object type for reference\n",
    "            \"auto\": \"previous_action\",   # old action\n",
    "            \"manual\": \"new_action\"         # new action\n",
    "        },\n",
    "        inplace=True\n",
    "    )\n",
    "    ov_action.to_csv(overrides_action_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b1cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before creating the JSON structure from the final database to be used in the D3 treemap,\n",
    "# I want to standardize the canonical_type values a bit more, since there are some categories that are very similar and will\n",
    "# clutter the visualization. I will do this by merging some of the categories into broader ones, following these criteria:\n",
    "# 1. create a table with all the unique canonical_type values and some other columns that will help decide how to group them)\n",
    "# 2. use a mix of LLM prompting and manual review to define broader categories\n",
    "# 3. create a mapping dictionary from the table and apply it to the final database\n",
    "\n",
    "# This further canonization is done for display purposes.\n",
    "# At this stage the action_family column has already been assigned using the\n",
    "# original, more specific object types (e.g. \"tinderboxes\", \"ballot boxes\").\n",
    "# Changing the canonical_type now will not affect the verbs/actions logic;\n",
    "# it only merges similar objects (e.g. all bowls → \"bowls\") so the treemap\n",
    "# labels are cleaner and easier to read.\n",
    "\n",
    "# grouping by unique canonical_type and keeping one representative row for each type by adding some additional columns with details\n",
    "unique_canon_df = ( \n",
    "    df.groupby(\"canonical_type\", as_index=False)\n",
    "    .first()[[\"canonical_type\", \"object_types_list\", \"objectType\", \"physicalDescription\"]] \n",
    "    )\n",
    "\n",
    "# saving the file to review and further canonization, where possible, with the support of an LLM\n",
    "unique_canon_df.to_csv(Path(\"unique_canonical_types_for_review.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ecfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the necessary CSV files\n",
    "df_with_action = pd.read_csv(\"final_mapped_database_manual.csv\")\n",
    "canon = pd.read_csv(\"unique_canonical_types_canonized.csv\")\n",
    "\n",
    "# build a mapping {original -> suggested}\n",
    "mapping = dict(zip(canon[\"canonical_type\"], canon[\"suggested_canonical\"]))\n",
    "\n",
    "df_canonized = df_with_action.copy()\n",
    "df_canonized[\"canonical_type\"] = (\n",
    "    df_canonized[\"canonical_type\"].map(mapping).fillna(df_canonized[\"canonical_type\"])\n",
    ")\n",
    "\n",
    "df_canonized.to_csv(\"final_mapped_database_canonized.csv\", index=False)\n",
    "\n",
    "# final corrections\n",
    "df_with_action_canonized = pd.read_csv(\"final_mapped_database_canonized_manualfix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47772f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replicating the same process as above to document the final manual fixes\n",
    "# comparing the original vs manually-fixed CSVs to capture every change made by hand,\n",
    "# and exporting those differences as \"override\" tables\n",
    "\n",
    "\n",
    "# 1) input / output paths\n",
    "\n",
    "# input\n",
    "auto_file = Path(\"final_mapped_database_canonized.csv\") # before final manual fixes\n",
    "manual_file = Path(\"final_mapped_database_canonized_manualfix.csv\") # after final manual fixes\n",
    "\n",
    "# output\n",
    "overrides_canonical_file = Path(\"manual_overrides_canonical_final.csv\")\n",
    "overrides_action_file = Path(\"manual_overrides_action_final.csv\")\n",
    "audit_rowlevel_file = Path(\"manual_overrides_rowlevel_final.csv\")\n",
    "\n",
    "# 2) read both CSVs\n",
    "auto_df = pd.read_csv(auto_file)\n",
    "manual_df = pd.read_csv(manual_file)\n",
    "\n",
    "# 3) fixed join key and other important columns\n",
    "id_col = \"EDANurl\"\n",
    "group_col = \"object_types_str\"\n",
    "fields_to_compare = [\"canonical_type\", \"action_family\"]\n",
    "\n",
    "# 4) merge so each row shows auto + manual side by side\n",
    "merged = auto_df[[id_col, group_col] + fields_to_compare] \\\n",
    "           .merge(\n",
    "               manual_df[[id_col, group_col] + fields_to_compare],\n",
    "               on=[id_col, group_col],\n",
    "               suffixes=(\"_auto\", \"_manual\"),\n",
    "               how=\"inner\"\n",
    "           )\n",
    "\n",
    "# 5) detect changed rows\n",
    "changed_mask = False\n",
    "for f in fields_to_compare:\n",
    "    changed = merged[f\"{f}_auto\"].astype(str).fillna(\"\") != \\\n",
    "              merged[f\"{f}_manual\"].astype(str).fillna(\"\")\n",
    "    changed_mask = changed if isinstance(changed_mask, bool) else (changed_mask | changed)\n",
    "\n",
    "changed_rows = merged[changed_mask].copy()\n",
    "changed_rows.to_csv(audit_rowlevel_file, index=False)\n",
    "\n",
    "# 6) build one-to-one overrides by original label\n",
    "def build_override(field):\n",
    "    a, m = f\"{field}_auto\", f\"{field}_manual\"\n",
    "    g = merged.groupby(group_col)[[a, m]] \\\n",
    "              .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else s.iloc[0]) \\\n",
    "              .reset_index().rename(columns={a: \"auto\", m: \"manual\"})\n",
    "    return g[g[\"auto\"].astype(str).fillna(\"\") != g[\"manual\"].astype(str).fillna(\"\")]\n",
    "\n",
    "ov_canonical = build_override(\"canonical_type\")\n",
    "ov_action    = build_override(\"action_family\")\n",
    "\n",
    "if not ov_canonical.empty:\n",
    "    ov_canonical.rename(\n",
    "        columns={\n",
    "            group_col: \"original_label\",\n",
    "            \"manual\": \"canonical_type_manual\"\n",
    "        },\n",
    "        inplace=True\n",
    "    )\n",
    "    ov_canonical[[\"original_label\", \"canonical_type_manual\"]] \\\n",
    "        .to_csv(overrides_canonical_file, index=False)\n",
    "\n",
    "if not ov_action.empty:\n",
    "    ov_action.rename(\n",
    "        columns={\n",
    "            group_col: \"object_type\", # keep object type for reference\n",
    "            \"auto\": \"previous_action\", # old action\n",
    "            \"manual\": \"new_action\"     # new action\n",
    "        },\n",
    "        inplace=True\n",
    "    )\n",
    "    ov_action.to_csv(overrides_action_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15952a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the nested JSON structure for a D3 treemap\n",
    "\n",
    "df_with_action_canonized = pd.read_csv(\"final_mapped_database_canonized_manualfix.csv\")\n",
    "\n",
    "# keeping only what is needed from the final database\n",
    "base = (\n",
    "    df_with_action_canonized[[\"action_family\", \"canonical_type\"]]\n",
    ")\n",
    "\n",
    "# aggregating counts: one row per (action_family, canonical_type) with a count\n",
    "#    - .size() counts how many records fall into each pair of keys\n",
    "#    - renamig that \"size\" column to \"count\" for clarity\n",
    "counts = (\n",
    "    base\n",
    "    .groupby([\"action_family\", \"canonical_type\"], as_index=False)\n",
    "    .size()\n",
    "    .rename(columns={\"size\": \"count\"})\n",
    ")\n",
    "\n",
    "# sorting action families in alphabetical order, then object types by descending count\n",
    "counts = counts.sort_values(\n",
    "    by=[\"action_family\", \"count\", \"canonical_type\"],\n",
    "    ascending=[True, False, True],\n",
    ")\n",
    "\n",
    "# building the nested structure that the D3 treemap expects:\n",
    "#    {\n",
    "#      \"name\": \"root\",\n",
    "#      \"children\": [\n",
    "#        {\n",
    "#          \"name\": \"<action_family>\",\n",
    "#          \"children\": [\n",
    "#            { \"name\": \"<canonical_type>\", \"value\": <count> },\n",
    "#            ...\n",
    "#          ]\n",
    "#        },\n",
    "#        ...\n",
    "#      ]\n",
    "#    }\n",
    "\n",
    "# the root is the top-level folder that contains everything (the database)\n",
    "# each action_family is a child of the root\n",
    "# each canonical_type is a child of the action_family\n",
    "# each canonical_type has a value, which is its count (the treemap’s rectangles are sized by these values)\n",
    "\n",
    "data = {\"name\": \"root\", \"children\": []}\n",
    "\n",
    "# grouping the counts by action_family\n",
    "for action_family, group_df in counts.groupby(\"action_family\"):\n",
    "    # for each canonical_type inside this action_family, make a leaf node\n",
    "    children = [\n",
    "        {\n",
    "            \"name\": row[\"canonical_type\"],\n",
    "            \"value\": int(row[\"count\"])\n",
    "        }\n",
    "        for _, row in group_df.iterrows()\n",
    "    ]\n",
    "\n",
    "    # adding this action_family (internal node) to the root’s children\n",
    "    data[\"children\"].append({\n",
    "        \"name\": action_family,\n",
    "        \"children\": children\n",
    "    })\n",
    "\n",
    "# saving the dictionary as a JSON file\n",
    "# creating a path pointing to the output file\n",
    "out_path = Path(\"treemap_data.json\")\n",
    "# converting the Python dictionary into a JSON string\n",
    "out_path.write_text(\n",
    "    json.dumps(data, ensure_ascii=False, indent=2), \n",
    "    encoding=\"utf-8\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf23aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating another JSON structure for the 3rd click level that maps each object type (column canonical_type)\n",
    "# with its list of individual records (title, unitCode, collectionsURL)\n",
    "\n",
    "#  this will be its shape:\n",
    "\n",
    "#   {\n",
    "#     \"chair\": [\n",
    "#       {\"title\": \"...\", \"unitCode\": \"...\", \"collectionsURL\": \"...\"},\n",
    "#       {\"title\": \"...\", \"unitCode\": \"...\", \"collectionsURL\": \"...\"}\n",
    "#     ],\n",
    "#     \"spoon\": [\n",
    "#       {\"title\": \"...\", \"unitCode\": \"...\", \"collectionsURL\": \"...\"}\n",
    "#     ]\n",
    "#   }\n",
    "\n",
    "\n",
    "# choosing the columns that will be used\n",
    "col_object_type = \"canonical_type\" # this will be the grouping key\n",
    "col_title = \"title\"           \n",
    "col_unit = \"unitCode\"    \n",
    "col_url = \"collectionsURL\"\n",
    "col_identifier = \"EDANurl\"\n",
    "col_action = \"action_family\" # this column will be used to filter the details panel not only by object type but also by action family\n",
    "\n",
    "# preparing an empty dictionary to fill as the function iterates over rows\n",
    "lookup = {}\n",
    "\n",
    "# loop over every row in df_with_action\n",
    "for _, row in df_with_action_canonized.iterrows():\n",
    "    # get the object type for this row\n",
    "    obj_type = row[col_object_type]\n",
    "    # if the object type column is empty, skip it (just in case)\n",
    "    if not obj_type:\n",
    "        continue\n",
    "    # create the small record we want to store\n",
    "    item = {\n",
    "        \"title\": row[col_title],\n",
    "        \"unitCode\": row[col_unit],\n",
    "        \"collectionsURL\": row[col_url],\n",
    "        \"EDANurl\": row[col_identifier],\n",
    "        \"action_family\": row[col_action],\n",
    "    }\n",
    "    # if the object type on the current row hasn’t appeared yet, create an empty list for it\n",
    "    # this is to populate the lookup dictionary with every object type once (the first time it appears)     \n",
    "    if obj_type not in lookup:\n",
    "        lookup[obj_type] = []\n",
    "    # and then append the record on the current row to the list for the current object type\n",
    "    lookup[obj_type].append(item)\n",
    "\n",
    "# saving the dictionary as a JSON file\n",
    "# creating a path pointing to the output file\n",
    "out_path = Path(\"object_details.json\")\n",
    "# converting the Python dictionary into a JSON string\n",
    "out_path.write_text(\n",
    "    json.dumps(lookup, ensure_ascii=False, indent=2), \n",
    "    encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25411899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
